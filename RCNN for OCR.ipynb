{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "from xml.dom import minidom\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pandas import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse('MiracleDataFile.xml')\n",
    "itemlist = xmldoc.getElementsByTagName('word')\n",
    "\n",
    "    # In[67]:\n",
    "\n",
    "\n",
    "    #images_labels = []\n",
    "    #images_labels.append([]) #making a 2D array in python\n",
    "    #images_labels.append([])\n",
    "    \n",
    "tags = []\n",
    "words = []\n",
    "for z in itemlist:\n",
    "    tags.append(z.attributes['id'].value)\n",
    "    words.append(z.attributes['text'].value)\n",
    "\n",
    "    #print(store)\n",
    "df = DataFrame({'Word Id': tags, 'Text': words})\n",
    "df = DataFrame.drop_duplicates(df)\n",
    "df = df.sort_values(by = ['Word Id'], ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [] \n",
    "imgs = []\n",
    "\n",
    "image_id_real = []\n",
    "\n",
    "for file in os.listdir('C:/Users/Adrian/Desktop/SparkDevAI2018/TestSetWords/TestSetWords'):\n",
    "        for file2 in os.listdir('C:/Users/Adrian/Desktop/SparkDevAI2018/TestSetWords/TestSetWords/' + file):\n",
    "            for image in os.listdir('C:/Users/Adrian/Desktop/SparkDevAI2018/TestSetWords/TestSetWords/' + file + '/' + file2):\n",
    "                imgs.append(cv2.imread('C:/Users/Adrian/Desktop/SparkDevAI2018/TestSetWords/TestSetWords/' + file + '/' + file2+ '/'+image))\n",
    "                image_id = image.split('.')\n",
    "                image_id_real.append(image_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index in image_id_real:\n",
    "    row = df[df['Word Id'].str.match(index)]\n",
    "    word = row.iloc[0]['Text']\n",
    "    labels.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "letters = ''\n",
    "\n",
    "for x in labels:\n",
    "    letters += x\n",
    "\n",
    "letters = \"\".join(set(letters))\n",
    "letters = \"\".join(sorted(letters))\n",
    "\n",
    "print(len(letters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labels_to_text(labels):\n",
    "    return ''.join(list(map(lambda x: letters[int(x)], labels)))\n",
    "\n",
    "def text_to_labels(text):\n",
    "    return list(map(lambda x: letters.index(x), text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4824, 64, 128)\n"
     ]
    }
   ],
   "source": [
    "img_data = []\n",
    "\n",
    "for img in imgs:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(img, (128, 64))\n",
    "    data = np.asarray(img, dtype = \"int32\")\n",
    "    #data /= 255\n",
    "    img_data.append(data)\n",
    "\n",
    "img_data = np.array(img_data)\n",
    "print(img_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([21]) array([33, 35, 41, 25]) array([63, 58]) ...\n",
      " array([21, 62, 62, 52, 62, 63, 44, 57, 46, 48])\n",
      " array([50, 61, 44, 57, 63, 62]) array([8])]\n"
     ]
    }
   ],
   "source": [
    "labels_encoded = []\n",
    "for word in labels:\n",
    "    word = text_to_labels(word)\n",
    "    word = np.asarray(word, dtype = \"int32\")\n",
    "    labels_encoded.append(word)\n",
    "labels_encoded = np.array(labels_encoded)\n",
    "\n",
    "print(labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(max(labels_encoded, key = tuple)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into X and Y Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4100,)\n"
     ]
    }
   ],
   "source": [
    "X, y = img_data, labels_encoded\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= .15)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D #convolution 2d for images\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D #for pooling\n",
    "from keras.layers import Flatten #for flattening the pooling into the inputs of full layer\n",
    "from keras.layers import Dense #to create hidden layers, into ann\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def functional():\n",
    "    img_h = 64\n",
    "    img_w = 128\n",
    "    # Network parameters\n",
    "    conv_filters = 16\n",
    "    kernel_size = (3, 3)\n",
    "    pool_size = 2\n",
    "    time_dense_size = 32\n",
    "    rnn_size = 512\n",
    "        \n",
    "    batch_size = 32\n",
    "    downsample_factor = pool_size ** 2\n",
    "\n",
    "    act = 'relu'\n",
    "    input_data = Input(name='the_input', shape=(img_w, img_h, 1), dtype='float32')\n",
    "    inner = Conv2D(conv_filters, kernel_size, padding='same',\n",
    "                   activation=act, kernel_initializer='he_normal',\n",
    "                   name='conv1')(input_data)\n",
    "    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner)\n",
    "    inner = Conv2D(conv_filters, kernel_size, padding='same',\n",
    "                   activation=act, kernel_initializer='he_normal',\n",
    "                   name='conv2')(inner)\n",
    "    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner)\n",
    "\n",
    "    conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters)\n",
    "    inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n",
    "\n",
    "    # cuts down input size going into RNN:\n",
    "    inner = Dense(time_dense_size, activation=act, name='dense1')(inner)\n",
    "\n",
    "    # Two layers of bidirecitonal GRUs\n",
    "    # GRU seems to work as well, if not better than LSTM:\n",
    "    gru_1 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru1')(inner)\n",
    "    gru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru1_b')(inner)\n",
    "    gru1_merged = add([gru_1, gru_1b])\n",
    "    gru_2 = GRU(rnn_size, return_sequences=True, kernel_initializer='he_normal', name='gru2')(gru1_merged)\n",
    "    gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True, kernel_initializer='he_normal', name='gru2_b')(gru1_merged)\n",
    "\n",
    "    # transforms RNN output to character activations:\n",
    "    inner = Dense(71, kernel_initializer='he_normal',\n",
    "                  name='dense2')(concatenate([gru_2, gru_2b]))\n",
    "    y_pred = Activation('softmax', name='softmax')(inner)\n",
    "    Model(inputs=input_data, outputs=y_pred).summary()\n",
    "\n",
    "    labels = Input(name='the_labels', shape=[9], dtype='float32')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    # Keras doesn't currently support loss funcs with extra parameters\n",
    "    # so CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n",
    "\n",
    "    # clipnorm seems to speeds up convergence\n",
    "    sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "\n",
    "    model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    # the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
    "    \n",
    "    \n",
    "    # captures output of softmax so we can decode the output during visualization\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(img_w, img_h, max_text_len):\n",
    "    #Parameters\n",
    "    conv_filters = 16\n",
    "    kernel_size = (3, 3)\n",
    "    pool_size = 2\n",
    "    time_dense_size = 32\n",
    "    rnn_size = 512\n",
    "    #Initializing CNN\n",
    "    classifier = Sequential()\n",
    "    \n",
    "    #Step 1\n",
    "    classifier.add(Convolution2D(conv_filters, kernel_size, input_shape=(img_w, img_h, 1), padding = 'same', activation=\"relu\",  kernel_initializer='he_normal'))\n",
    "    \n",
    "    #print(classifier.layers[0].shape)\n",
    "    #Step2 MAXadd(Convolution2D(32, (3, 3), activation=\"relu\"))\n",
    "    #POOLING\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #to make more accurate results, another convolutional layer.\n",
    "    classifier.add(Convolution2D(conv_filters, kernel_size, padding = 'same', activation=\"relu\", kernel_initializer='he_normal'))\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_filters)\n",
    "    \n",
    "    #print(conv_to_rnn_dims)\n",
    "    \n",
    "    classifier.add(Reshape(target_shape=conv_to_rnn_dims))\n",
    "    \n",
    "    #1 par how many neurons in hidden layer 128 gotten from experimentation more than 100\n",
    "    classifier.add(Dense(units = 32, activation = 'relu'))\n",
    "        \n",
    "    # Adding the first LSTM layer and some Dropout regularisation\n",
    "    classifier.add(Bidirectional(LSTM(units = 512, return_sequences = True, kernel_initializer='he_normal')))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    \n",
    "    # Adding a second LSTM layer and some Dropout regularisation\n",
    "    classifier.add(Bidirectional(LSTM(units = 512, return_sequences = True, kernel_initializer='he_normal')))\n",
    "    classifier.add(Dropout(0.2))\n",
    "    \n",
    "    classifier.add(Flatten())\n",
    "    # Adding the output layer\n",
    "    classifier.add(Dense(units = len(letters)+1, activation = 'softmax', kernel_initializer='he_normal'))\n",
    "    #classifier.add(Activation('softmax'))\n",
    "    \n",
    "    #labels = Input(name='the_labels', shape=[max_text_len], dtype='float32')\n",
    "    #print(labels)\n",
    "    #input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    #print(input_length)\n",
    "    #label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    #print(label_length)\n",
    "    \n",
    "    \n",
    "    print(classifier.summary())\n",
    "    \n",
    "    #intermediate_layer_model = Model(inputs=classifier.input, outputs=classifier.layers[11].output)\n",
    "    #y_pred = classifier.layers[11].get_output_at(0)\n",
    "\n",
    "    #y_pred = classifier.layers[11].get_output_at(0)\n",
    "\n",
    "    #print(y_pred)\n",
    "    \n",
    "    #classifier.add(Lambda(ctc_lambda_func(y_pred, labels, input_length, label_length), output_shape=(1,)))\n",
    "    \n",
    "    sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
    "    \n",
    "    #model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)\n",
    "    # Compiling\n",
    "    #classifier.compile(loss= {'ctc': lambda y_true, y_pred: y_pred}, optimizer= sgd)\n",
    "    classifier.compile(loss = 'categorical_crossentropy', optimizer = sgd)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Let's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 128, 64, 16)       160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 64, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 64, 32, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 32, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32, 32)            8224      \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 32, 1024)          2232320   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32, 1024)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 32, 1024)          6295552   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32, 1024)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 71)                2326599   \n",
      "=================================================================\n",
      "Total params: 10,865,175\n",
      "Trainable params: 10,865,175\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model(128, 64, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4100, 128, 64, 1)\n",
      "(4100,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-237889038cb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0menc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Key here is sparse=False!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   2017\u001b[0m         \"\"\"\n\u001b[0;32m   2018\u001b[0m         return _transform_selected(X, self._fit_transform,\n\u001b[1;32m-> 2019\u001b[1;33m                                    self.categorical_features, copy=True)\n\u001b[0m\u001b[0;32m   2020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2021\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36m_transform_selected\u001b[1;34m(X, transform, selected, copy)\u001b[0m\n\u001b[0;32m   1807\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msparse\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1808\u001b[0m     \"\"\"\n\u001b[1;32m-> 1809\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mselected\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                       force_all_finite)\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "print (X_train.shape)\n",
    "X_train = np.reshape(X_train, (4100, 128, 64, 1))\n",
    "print (y_train.shape)\n",
    "enc = OneHotEncoder(sparse=False) # Key here is sparse=False!\n",
    "y_train = enc.fit_transform(y_train.reshape((y_train.shape[0]),1))\n",
    "\n",
    "\n",
    "#y_train = np.array(y_train)\n",
    "#print(X_train)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
